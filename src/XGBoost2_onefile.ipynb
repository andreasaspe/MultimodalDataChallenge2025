{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b2baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique target classes: 181\n",
      "Target classes: ['0.0' '1.0' '10.0' '100.0' '101.0' '102.0' '103.0' '104.0' '105.0'\n",
      " '106.0']...\n",
      "\n",
      "Training set size: (2537, 4)\n",
      "Test set size: (635, 4)\n",
      "\n",
      "Training XGBoost model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awias/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [14:21:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:5.00084\n",
      "[1]\tvalidation_0-mlogloss:4.86021\n",
      "[2]\tvalidation_0-mlogloss:4.76831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Add validation set for early stopping\u001b[39;00m\n\u001b[32m     73\u001b[39m X_train_split, X_val, y_train_split, y_val = train_test_split(\n\u001b[32m     74\u001b[39m     X_train, y_train, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_train\n\u001b[32m     75\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     84\u001b[39m y_pred = xgb_model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/sklearn.py:1683\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1661\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1662\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1663\u001b[39m )\n\u001b[32m   1664\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1680\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1681\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/standard/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "data_file = \"/home/awias/data/Summerschool_2025/metadata_more.csv\"\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Only keep the records where the substring 'train' is in filename_index\n",
    "df = df[df['filename_index'].str.contains('train', na=False)].copy()\n",
    "\n",
    "# Remove rows where taxonID_index is missing (since it's our target)\n",
    "df = df.dropna(subset=['taxonID_index']).copy()\n",
    "\n",
    "# Remove 'eventDate' column\n",
    "df = df.drop(columns=['eventDate'], errors='ignore')\n",
    "\n",
    "# Remove rows where at least two of the specified columns are NaN\n",
    "cols_to_check = ['Habitat', 'Latitude', 'Longitude', 'Substrate']\n",
    "df = df[df[cols_to_check].isnull().sum(axis=1) < 2].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_features = ['Habitat', 'Substrate']\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "# Prepare final feature matrix (without date features)\n",
    "feature_cols_final = ['Habitat_encoded', 'Latitude', 'Longitude', 'Substrate_encoded']\n",
    "\n",
    "X = df[feature_cols_final]\n",
    "y = df['taxonID_index']\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y.astype(str))\n",
    "\n",
    "print(f\"\\nNumber of unique target classes: {len(target_encoder.classes_)}\")\n",
    "print(f\"Target classes: {target_encoder.classes_[:10]}...\")  # Show first 10\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Train XGBoost model with GPU support\n",
    "print(\"\\nTraining XGBoost model\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss' if len(np.unique(y_encoded)) > 2 else 'logloss',\n",
    "    tree_method='gpu_hist',  # Use GPU for training with gpu_hist\n",
    "    gpu_id=0,  # Use first GPU\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Add validation set for early stopping\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Feature Importance Analysis\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, y='feature', x='importance')\n",
    "plt.title('XGBoost Feature Importance for Fungi Classification')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# More detailed analysis\n",
    "print(\"\\nDetailed Feature Analysis:\")\n",
    "for idx, row in importance_df.iterrows():\n",
    "    feature = row['feature']\n",
    "    importance = row['importance']\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Show some statistics for this feature\n",
    "    if feature in X.columns:\n",
    "        feature_stats = X[feature].describe()\n",
    "        print(f\"  Min: {feature_stats['min']:.2f}, Max: {feature_stats['max']:.2f}, Mean: {feature_stats['mean']:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Create a correlation matrix for numerical features\n",
    "numerical_features = ['Latitude', 'Longitude']\n",
    "if len(numerical_features) > 1:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlation_matrix = X[numerical_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print decoded categorical mappings for interpretation\n",
    "print(\"\\nCategorical Feature Mappings:\")\n",
    "for col in categorical_features:\n",
    "    le = label_encoders[col]\n",
    "    mapping = dict(zip(range(len(le.classes_)), le.classes_))\n",
    "    print(f\"\\n{col}:\")\n",
    "    for code, category in mapping.items():\n",
    "        print(f\"  {code}: {category}\")\n",
    "\n",
    "# Show target distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "target_counts = pd.Series(y_encoded).value_counts().sort_index()\n",
    "plt.bar(range(len(target_counts)), target_counts.values)\n",
    "plt.title('Distribution of Target Classes')\n",
    "plt.xlabel('Encoded Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(0, len(target_counts), max(1, len(target_counts)//20)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget distribution summary:\")\n",
    "print(f\"Total classes: {len(target_counts)}\")\n",
    "print(f\"Most common class frequency: {target_counts.max()}\")\n",
    "print(f\"Least common class frequency: {target_counts.min()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
